#########################################################################################################
# Logisland configuration script tempate
#########################################################################################################

version: 0.10
documentation: LogIsland analytics main config file. Put here every engine or component config

#########################################################################################################
# engine
engine:
  component: com.hurence.logisland.engine.spark.KafkaStreamProcessingEngine
  type: engine
  documentation: Index some apache logs with logisland
  configuration:
    spark.app.name: IndexBiNetflowDemo
    spark.master: yarn-cluster
    spark.driver.memory: 1G
    spark.driver.cores: 1
    spark.executor.memory: 1G
    spark.executor.instances: 5
    spark.executor.cores: 2
    spark.yarn.queue: default
    spark.yarn.maxAppAttempts: 4
    spark.yarn.am.attemptFailuresValidityInterval: 1h
    spark.yarn.max.executor.failures: 20
    spark.yarn.executor.failuresValidityInterval: 1h
    spark.task.maxFailures: 8
    spark.serializer: org.apache.spark.serializer.KryoSerializer
    spark.streaming.batchDuration: 10000
    spark.streaming.backpressure.enabled: false
    spark.streaming.unpersist: false
    spark.streaming.blockInterval: 500
    spark.streaming.kafka.maxRatePerPartition: 3000
    spark.streaming.timeout: -1
    spark.streaming.unpersist: false
    spark.streaming.kafka.maxRetries: 30
    spark.streaming.ui.retainedBatches: 200
    spark.streaming.receiver.writeAheadLog.enable: false
    spark.ui.port: 4050

  streamConfigurations:

    - stream: parsing_stream
      component: com.hurence.logisland.stream.spark.KafkaRecordStreamParallelProcessing
      type: stream
      documentation: a processor that links
      configuration:
        kafka.input.topics: binetflow_raw
        kafka.output.topics: binetflow_events
        kafka.error.topics: _errors
        kafka.input.topics.serializer: none
        kafka.output.topics.serializer: com.hurence.logisland.serializer.KryoSerializer
        kafka.error.topics.serializer: com.hurence.logisland.serializer.JsonSerializer
        kafka.metadata.broker.list: sd-84190:6667,sd-84191:6667,sd-84192:6667,sd-84186:6667
        kafka.zookeeper.quorum: sd-76387:2181,sd-84186:2181,sd-84189:2181
        kafka.topic.autoCreate: true
        kafka.topic.default.partitions: 20
        kafka.topic.default.replicationFactor: 3
        kafka.message.key.field: record_id
      processorConfigurations:

        - processor: bi_netflow_parser
          component: com.hurence.logisland.processor.SplitText
          type: parser
          documentation: a parser that produce events from bi-directionnal netflow logs
          configuration:
            record.type: bi_netflow
            value.regex: (\d{4}\/\d{2}\/\d{2}\s\d{1,2}:\d{1,2}:\d{1,2}\.\d{0,6}),([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,([^,]+)?,flow=([^,]+)
            value.fields: timestamp,duration,protocol,src_ip,src_port,direction,dest_ip,dest_port,state,src_tos,dest_tos,packets_out,bytes_out,bytes_in,label

        - processor: field_types_converter
          component: com.hurence.logisland.processor.ConvertFieldsType
          type: processor
          documentation: convert some field to a given type
          configuration:
            bytes_in: long
            bytes_out: long
            packets_out: long
            duration: float
            src_tos: int
            dest_tos: int

        - processor: date_updater
          component: com.hurence.logisland.processor.UpdateBiNetflowDate
          type: processor
          documentation: compute record_time

        - processor: id_modifier
          component: com.hurence.logisland.processor.ModifyId
          type: processor
          documentation: convert some field to a given type
          configuration:
            id.generation.strategy: fromFields
            fields.to.hash: record_time,src_ip,dest_ip
            java.formatter.string: "%s-%s-%s"